{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Traing RL Agent to play PONG using NN and Policy Gradients"
      ],
      "metadata": {
        "id": "mMxkvPDdY45Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 1.** Checking the available GPU devices"
      ],
      "metadata": {
        "id": "lZXYDHUBZKdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NIHaVke5ZYq4",
        "outputId": "86442ab6-0354-44c0-ef8a-45af593e2ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSsYXaoxZdw_",
        "outputId": "ff227839-4666-4297-b01a-550529a6a62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar  6 19:14:41 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    32W /  70W |   7095MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 2.** Installing the required libraries/packages"
      ],
      "metadata": {
        "id": "Yxt0WuUNZeV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari]\n",
        "!pip install gym[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "Xsx9KiFvZwBY",
        "outputId": "a4f550a6-4924-40d6-b6fe-d9383551d535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (2.2.1)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.15.0)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license]) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license]) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.25.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.5.5.tar.gz (22 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license]) (3.15.0)\n",
            "Collecting libtorrent\n",
            "  Using cached libtorrent-2.0.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.15)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2022.12.7)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.5.5-py3-none-any.whl size=448855 sha256=8a0e01b6d4abae93a74f7276349a31aa1872fd01b5d996b145e5a5c7196b517d\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/da/0f/2171aa233043791a9e688909cbafe26266cb4fe1d2e8ff3ff9\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: libtorrent, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.5.5 autorom-0.4.2 libtorrent-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 3.** Importing the required libraries "
      ],
      "metadata": {
        "id": "sMyIAmHxaJV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "import glob\n",
        "import os\n"
      ],
      "metadata": {
        "id": "B8fOlSRXaKdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 4.** Setting the random seed"
      ],
      "metadata": {
        "id": "bAXH6uhwaKqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "Ib1ZLEdBaK2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 5.** Defining the hyperparameters"
      ],
      "metadata": {
        "id": "hjLj-SHyaLJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "discount_factor = 0.99\n",
        "num_episodes = 3000\n",
        "num_steps = 10000\n",
        "render_env = False"
      ],
      "metadata": {
        "id": "Zy0d92ktaLS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 6.** Defining the optimizer"
      ],
      "metadata": {
        "id": "COkLhUPIdQJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate)"
      ],
      "metadata": {
        "id": "cvswwFNKdQRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 7.** Initializing the gym PONG environment"
      ],
      "metadata": {
        "id": "GM0i9N1cdQX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Pong-v0')\n",
        "state_size = env.observation_space.shape\n",
        "action_size = env.action_space.n"
      ],
      "metadata": {
        "id": "VV0cXvMsdQfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 7.** Building the NN Model and compiling"
      ],
      "metadata": {
        "id": "R2Y9IKrVdQlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, activation='relu', input_shape=state_size),\n",
        "  tf.keras.layers.Conv2D(32, kernel_size=4, strides=2, activation='relu'),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dense(action_size, activation='softmax')\n",
        "])\n",
        "print(f'Model built from scratch')\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mugTzMQfdQsg",
        "outputId": "59830c97-4e8e-4792-d213-710a5372a9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built from scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zmxxme_F-9O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 8.** Defining the Training function"
      ],
      "metadata": {
        "id": "amrApqrudQzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function(reduce_retracing=True)\n",
        "def train_step(states, actions, discounted_rewards):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get action probabilities for given states\n",
        "        logits = model(states)\n",
        "        action_probabilities = tf.nn.softmax(logits)\n",
        "\n",
        "        # Compute loss\n",
        "        cross_entropy = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            actions, action_probabilities, from_logits=False)\n",
        "        loss = tf.reduce_mean(cross_entropy * discounted_rewards)\n",
        "\n",
        "    # Compute gradients and update weights\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "qXUrte7IdQ8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 8.** Defining the Training loop (Agent plays Pong) "
      ],
      "metadata": {
        "id": "hkKlRx8hdREO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('GPU:0'):\n",
        "    # Loop to train\n",
        "    for episode in range(num_episodes):\n",
        "        episode_rewards, episode_actions, episode_states = [], [], []\n",
        "        state = env.reset()\n",
        "        for step in range(num_steps):\n",
        "            if render_env:\n",
        "                env.render()\n",
        "\n",
        "            # Action probabilities for given state\n",
        "            state = np.expand_dims(state, axis=0).astype(np.float32)\n",
        "            logits = model(state)\n",
        "            action_probabilities = tf.nn.softmax(logits).numpy()[0]\n",
        "\n",
        "            # Sampling action from action probabilities\n",
        "            action = np.random.choice(action_size, p=action_probabilities)\n",
        "\n",
        "            # Performing action to fetch next state and reward\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Saving info for each episode \n",
        "            episode_rewards.append(reward)\n",
        "            episode_actions.append(action)\n",
        "            episode_states.append(state)\n",
        "\n",
        "            if done:\n",
        "                # Computing rewards-to-go\n",
        "                rewards_to_go = np.zeros_like(episode_rewards)\n",
        "                cumulative_reward = 0\n",
        "                for i in reversed(range(len(episode_rewards))):\n",
        "                    cumulative_reward = cumulative_reward * discount_factor + episode_rewards[i]\n",
        "                    rewards_to_go[i] = cumulative_reward\n",
        "\n",
        "                # Normalizing rewards-to-go\n",
        "                rewards_to_go -= np.mean(rewards_to_go)\n",
        "                rewards_to_go /= np.std(rewards_to_go)\n",
        "\n",
        "                # Training policy network using rewards-to-go\n",
        "                episode_states = np.concatenate(episode_states, axis=0)\n",
        "                episode_actions = np.array(episode_actions)\n",
        "                rewards_to_go = rewards_to_go.astype(np.float32)\n",
        "                train_step(episode_states, episode_actions, rewards_to_go)\n",
        "\n",
        "                # Printing reward info for each episode\n",
        "                print(\"Episode:\", episode + 1, \"Reward:\", sum(episode_rewards))\n",
        "                if (episode + 1) % 50 == 0:\n",
        "                    model.save(f'Pong_PolicyGrad_model_episode_{episode+1}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.h5')\n",
        "                break\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "\n",
        "# Close environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N30FOaRQdRMA",
        "outputId": "6e383b5a-d942-437f-cae8-57f420fa840e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1 Reward: -20.0\n",
            "Episode: 2 Reward: -20.0\n",
            "Episode: 3 Reward: -20.0\n",
            "Episode: 4 Reward: -20.0\n",
            "Episode: 5 Reward: -21.0\n",
            "Episode: 6 Reward: -20.0\n",
            "Episode: 7 Reward: -20.0\n",
            "Episode: 8 Reward: -21.0\n",
            "Episode: 9 Reward: -20.0\n",
            "Episode: 10 Reward: -20.0\n",
            "Episode: 11 Reward: -21.0\n",
            "Episode: 12 Reward: -20.0\n",
            "Episode: 13 Reward: -21.0\n",
            "Episode: 14 Reward: -21.0\n",
            "Episode: 15 Reward: -20.0\n",
            "Episode: 16 Reward: -21.0\n",
            "Episode: 17 Reward: -21.0\n",
            "Episode: 18 Reward: -21.0\n",
            "Episode: 19 Reward: -21.0\n",
            "Episode: 20 Reward: -20.0\n",
            "Episode: 21 Reward: -20.0\n",
            "Episode: 22 Reward: -20.0\n",
            "Episode: 23 Reward: -20.0\n",
            "Episode: 24 Reward: -20.0\n",
            "Episode: 25 Reward: -20.0\n",
            "Episode: 26 Reward: -21.0\n",
            "Episode: 27 Reward: -21.0\n",
            "Episode: 28 Reward: -20.0\n",
            "Episode: 29 Reward: -20.0\n",
            "Episode: 30 Reward: -21.0\n",
            "Episode: 31 Reward: -21.0\n",
            "Episode: 32 Reward: -20.0\n",
            "Episode: 33 Reward: -20.0\n",
            "Episode: 34 Reward: -21.0\n",
            "Episode: 35 Reward: -20.0\n",
            "Episode: 36 Reward: -21.0\n",
            "Episode: 37 Reward: -21.0\n",
            "Episode: 38 Reward: -19.0\n",
            "Episode: 39 Reward: -21.0\n",
            "Episode: 40 Reward: -21.0\n",
            "Episode: 41 Reward: -21.0\n",
            "Episode: 42 Reward: -21.0\n",
            "Episode: 43 Reward: -21.0\n",
            "Episode: 44 Reward: -20.0\n",
            "Episode: 45 Reward: -20.0\n",
            "Episode: 46 Reward: -20.0\n",
            "Episode: 47 Reward: -20.0\n",
            "Episode: 48 Reward: -21.0\n",
            "Episode: 49 Reward: -20.0\n",
            "Episode: 50 Reward: -20.0\n",
            "Episode: 51 Reward: -21.0\n",
            "Episode: 52 Reward: -19.0\n",
            "Episode: 53 Reward: -21.0\n",
            "Episode: 54 Reward: -21.0\n",
            "Episode: 55 Reward: -21.0\n",
            "Episode: 56 Reward: -21.0\n",
            "Episode: 57 Reward: -21.0\n",
            "Episode: 58 Reward: -20.0\n",
            "Episode: 59 Reward: -21.0\n",
            "Episode: 60 Reward: -21.0\n",
            "Episode: 61 Reward: -21.0\n",
            "Episode: 62 Reward: -21.0\n",
            "Episode: 63 Reward: -21.0\n",
            "Episode: 64 Reward: -20.0\n",
            "Episode: 65 Reward: -21.0\n",
            "Episode: 66 Reward: -19.0\n",
            "Episode: 67 Reward: -21.0\n",
            "Episode: 68 Reward: -21.0\n",
            "Episode: 69 Reward: -21.0\n",
            "Episode: 70 Reward: -21.0\n",
            "Episode: 71 Reward: -21.0\n",
            "Episode: 72 Reward: -19.0\n",
            "Episode: 73 Reward: -20.0\n",
            "Episode: 74 Reward: -20.0\n",
            "Episode: 75 Reward: -21.0\n",
            "Episode: 76 Reward: -21.0\n",
            "Episode: 77 Reward: -20.0\n",
            "Episode: 78 Reward: -20.0\n",
            "Episode: 79 Reward: -21.0\n",
            "Episode: 80 Reward: -20.0\n",
            "Episode: 81 Reward: -21.0\n",
            "Episode: 82 Reward: -21.0\n",
            "Episode: 83 Reward: -20.0\n",
            "Episode: 84 Reward: -21.0\n",
            "Episode: 85 Reward: -21.0\n",
            "Episode: 86 Reward: -20.0\n",
            "Episode: 87 Reward: -20.0\n",
            "Episode: 88 Reward: -21.0\n",
            "Episode: 89 Reward: -20.0\n",
            "Episode: 90 Reward: -20.0\n",
            "Episode: 91 Reward: -21.0\n",
            "Episode: 92 Reward: -20.0\n",
            "Episode: 93 Reward: -20.0\n",
            "Episode: 94 Reward: -21.0\n",
            "Episode: 95 Reward: -20.0\n",
            "Episode: 96 Reward: -20.0\n",
            "Episode: 97 Reward: -21.0\n",
            "Episode: 98 Reward: -20.0\n",
            "Episode: 99 Reward: -21.0\n",
            "Episode: 100 Reward: -19.0\n",
            "Episode: 101 Reward: -21.0\n",
            "Episode: 102 Reward: -21.0\n",
            "Episode: 103 Reward: -21.0\n",
            "Episode: 104 Reward: -20.0\n",
            "Episode: 105 Reward: -21.0\n",
            "Episode: 106 Reward: -21.0\n",
            "Episode: 107 Reward: -21.0\n",
            "Episode: 108 Reward: -21.0\n",
            "Episode: 109 Reward: -20.0\n",
            "Episode: 110 Reward: -21.0\n",
            "Episode: 111 Reward: -20.0\n",
            "Episode: 112 Reward: -21.0\n",
            "Episode: 113 Reward: -21.0\n",
            "Episode: 114 Reward: -21.0\n",
            "Episode: 115 Reward: -21.0\n",
            "Episode: 116 Reward: -21.0\n",
            "Episode: 117 Reward: -21.0\n",
            "Episode: 118 Reward: -21.0\n",
            "Episode: 119 Reward: -21.0\n",
            "Episode: 120 Reward: -21.0\n",
            "Episode: 121 Reward: -21.0\n",
            "Episode: 122 Reward: -20.0\n",
            "Episode: 123 Reward: -21.0\n",
            "Episode: 124 Reward: -19.0\n",
            "Episode: 125 Reward: -21.0\n",
            "Episode: 126 Reward: -20.0\n",
            "Episode: 127 Reward: -21.0\n",
            "Episode: 128 Reward: -20.0\n",
            "Episode: 129 Reward: -20.0\n",
            "Episode: 130 Reward: -20.0\n",
            "Episode: 131 Reward: -20.0\n",
            "Episode: 132 Reward: -20.0\n",
            "Episode: 133 Reward: -21.0\n",
            "Episode: 134 Reward: -21.0\n",
            "Episode: 135 Reward: -20.0\n",
            "Episode: 136 Reward: -20.0\n",
            "Episode: 137 Reward: -20.0\n",
            "Episode: 138 Reward: -20.0\n",
            "Episode: 139 Reward: -21.0\n",
            "Episode: 140 Reward: -21.0\n",
            "Episode: 141 Reward: -21.0\n",
            "Episode: 142 Reward: -20.0\n",
            "Episode: 143 Reward: -21.0\n",
            "Episode: 144 Reward: -21.0\n",
            "Episode: 145 Reward: -20.0\n",
            "Episode: 146 Reward: -20.0\n",
            "Episode: 147 Reward: -20.0\n",
            "Episode: 148 Reward: -21.0\n",
            "Episode: 149 Reward: -20.0\n",
            "Episode: 150 Reward: -20.0\n",
            "Episode: 151 Reward: -20.0\n",
            "Episode: 152 Reward: -21.0\n",
            "Episode: 153 Reward: -21.0\n",
            "Episode: 154 Reward: -21.0\n",
            "Episode: 155 Reward: -20.0\n",
            "Episode: 156 Reward: -21.0\n",
            "Episode: 157 Reward: -21.0\n",
            "Episode: 158 Reward: -20.0\n",
            "Episode: 159 Reward: -20.0\n",
            "Episode: 160 Reward: -21.0\n",
            "Episode: 161 Reward: -21.0\n",
            "Episode: 162 Reward: -20.0\n",
            "Episode: 163 Reward: -21.0\n",
            "Episode: 164 Reward: -20.0\n",
            "Episode: 165 Reward: -21.0\n",
            "Episode: 166 Reward: -21.0\n",
            "Episode: 167 Reward: -20.0\n",
            "Episode: 168 Reward: -20.0\n",
            "Episode: 169 Reward: -21.0\n",
            "Episode: 170 Reward: -21.0\n",
            "Episode: 171 Reward: -21.0\n",
            "Episode: 172 Reward: -20.0\n",
            "Episode: 173 Reward: -21.0\n",
            "Episode: 174 Reward: -20.0\n",
            "Episode: 175 Reward: -21.0\n",
            "Episode: 176 Reward: -21.0\n",
            "Episode: 177 Reward: -20.0\n",
            "Episode: 178 Reward: -20.0\n",
            "Episode: 179 Reward: -20.0\n",
            "Episode: 180 Reward: -21.0\n",
            "Episode: 181 Reward: -21.0\n",
            "Episode: 182 Reward: -21.0\n",
            "Episode: 183 Reward: -20.0\n",
            "Episode: 184 Reward: -20.0\n",
            "Episode: 185 Reward: -21.0\n",
            "Episode: 186 Reward: -21.0\n",
            "Episode: 187 Reward: -20.0\n",
            "Episode: 188 Reward: -20.0\n",
            "Episode: 189 Reward: -21.0\n",
            "Episode: 190 Reward: -21.0\n",
            "Episode: 191 Reward: -21.0\n",
            "Episode: 192 Reward: -20.0\n",
            "Episode: 193 Reward: -20.0\n",
            "Episode: 194 Reward: -20.0\n",
            "Episode: 195 Reward: -20.0\n",
            "Episode: 196 Reward: -20.0\n",
            "Episode: 197 Reward: -21.0\n",
            "Episode: 198 Reward: -21.0\n",
            "Episode: 199 Reward: -20.0\n",
            "Episode: 200 Reward: -20.0\n",
            "Episode: 201 Reward: -21.0\n",
            "Episode: 202 Reward: -21.0\n",
            "Episode: 203 Reward: -20.0\n",
            "Episode: 204 Reward: -20.0\n",
            "Episode: 205 Reward: -21.0\n",
            "Episode: 206 Reward: -21.0\n",
            "Episode: 207 Reward: -21.0\n",
            "Episode: 208 Reward: -21.0\n",
            "Episode: 209 Reward: -21.0\n",
            "Episode: 210 Reward: -21.0\n",
            "Episode: 211 Reward: -20.0\n",
            "Episode: 212 Reward: -21.0\n",
            "Episode: 213 Reward: -20.0\n",
            "Episode: 214 Reward: -20.0\n",
            "Episode: 215 Reward: -20.0\n",
            "Episode: 216 Reward: -21.0\n",
            "Episode: 217 Reward: -19.0\n",
            "Episode: 218 Reward: -20.0\n",
            "Episode: 219 Reward: -20.0\n",
            "Episode: 220 Reward: -21.0\n",
            "Episode: 221 Reward: -20.0\n",
            "Episode: 222 Reward: -20.0\n",
            "Episode: 223 Reward: -21.0\n",
            "Episode: 224 Reward: -21.0\n",
            "Episode: 225 Reward: -20.0\n",
            "Episode: 226 Reward: -20.0\n",
            "Episode: 227 Reward: -20.0\n",
            "Episode: 228 Reward: -21.0\n",
            "Episode: 229 Reward: -20.0\n",
            "Episode: 230 Reward: -21.0\n",
            "Episode: 231 Reward: -20.0\n",
            "Episode: 232 Reward: -20.0\n",
            "Episode: 233 Reward: -20.0\n",
            "Episode: 234 Reward: -21.0\n",
            "Episode: 235 Reward: -20.0\n",
            "Episode: 236 Reward: -21.0\n",
            "Episode: 237 Reward: -21.0\n",
            "Episode: 238 Reward: -21.0\n",
            "Episode: 239 Reward: -19.0\n",
            "Episode: 240 Reward: -21.0\n",
            "Episode: 241 Reward: -21.0\n",
            "Episode: 242 Reward: -20.0\n",
            "Episode: 243 Reward: -20.0\n",
            "Episode: 244 Reward: -21.0\n",
            "Episode: 245 Reward: -21.0\n",
            "Episode: 246 Reward: -20.0\n",
            "Episode: 247 Reward: -20.0\n",
            "Episode: 248 Reward: -21.0\n",
            "Episode: 249 Reward: -20.0\n",
            "Episode: 250 Reward: -21.0\n",
            "Episode: 251 Reward: -20.0\n",
            "Episode: 252 Reward: -20.0\n",
            "Episode: 253 Reward: -21.0\n",
            "Episode: 254 Reward: -21.0\n",
            "Episode: 255 Reward: -20.0\n",
            "Episode: 256 Reward: -20.0\n",
            "Episode: 257 Reward: -21.0\n",
            "Episode: 258 Reward: -21.0\n",
            "Episode: 259 Reward: -20.0\n",
            "Episode: 260 Reward: -21.0\n",
            "Episode: 261 Reward: -20.0\n",
            "Episode: 262 Reward: -21.0\n",
            "Episode: 263 Reward: -21.0\n",
            "Episode: 264 Reward: -21.0\n",
            "Episode: 265 Reward: -21.0\n",
            "Episode: 266 Reward: -20.0\n",
            "Episode: 267 Reward: -21.0\n",
            "Episode: 268 Reward: -20.0\n",
            "Episode: 269 Reward: -20.0\n",
            "Episode: 270 Reward: -20.0\n",
            "Episode: 271 Reward: -21.0\n",
            "Episode: 272 Reward: -21.0\n",
            "Episode: 273 Reward: -21.0\n",
            "Episode: 274 Reward: -21.0\n",
            "Episode: 275 Reward: -21.0\n",
            "Episode: 276 Reward: -20.0\n",
            "Episode: 277 Reward: -20.0\n",
            "Episode: 278 Reward: -20.0\n",
            "Episode: 279 Reward: -21.0\n",
            "Episode: 280 Reward: -20.0\n",
            "Episode: 281 Reward: -20.0\n",
            "Episode: 282 Reward: -21.0\n",
            "Episode: 283 Reward: -20.0\n",
            "Episode: 284 Reward: -21.0\n",
            "Episode: 285 Reward: -20.0\n",
            "Episode: 286 Reward: -20.0\n",
            "Episode: 287 Reward: -20.0\n",
            "Episode: 288 Reward: -21.0\n",
            "Episode: 289 Reward: -21.0\n",
            "Episode: 290 Reward: -21.0\n",
            "Episode: 291 Reward: -20.0\n",
            "Episode: 292 Reward: -20.0\n",
            "Episode: 293 Reward: -21.0\n",
            "Episode: 294 Reward: -21.0\n",
            "Episode: 295 Reward: -21.0\n",
            "Episode: 296 Reward: -20.0\n",
            "Episode: 297 Reward: -20.0\n",
            "Episode: 298 Reward: -20.0\n",
            "Episode: 299 Reward: -19.0\n",
            "Episode: 300 Reward: -21.0\n",
            "Episode: 301 Reward: -21.0\n",
            "Episode: 302 Reward: -21.0\n",
            "Episode: 303 Reward: -21.0\n",
            "Episode: 304 Reward: -21.0\n",
            "Episode: 305 Reward: -20.0\n",
            "Episode: 306 Reward: -21.0\n",
            "Episode: 307 Reward: -20.0\n",
            "Episode: 308 Reward: -21.0\n",
            "Episode: 309 Reward: -21.0\n",
            "Episode: 310 Reward: -21.0\n",
            "Episode: 311 Reward: -21.0\n",
            "Episode: 312 Reward: -21.0\n",
            "Episode: 313 Reward: -20.0\n",
            "Episode: 314 Reward: -21.0\n",
            "Episode: 315 Reward: -20.0\n",
            "Episode: 316 Reward: -20.0\n",
            "Episode: 317 Reward: -19.0\n",
            "Episode: 318 Reward: -21.0\n",
            "Episode: 319 Reward: -21.0\n",
            "Episode: 320 Reward: -21.0\n",
            "Episode: 321 Reward: -21.0\n",
            "Episode: 322 Reward: -21.0\n",
            "Episode: 323 Reward: -21.0\n",
            "Episode: 324 Reward: -21.0\n",
            "Episode: 325 Reward: -21.0\n",
            "Episode: 326 Reward: -20.0\n",
            "Episode: 327 Reward: -21.0\n",
            "Episode: 328 Reward: -20.0\n",
            "Episode: 329 Reward: -20.0\n",
            "Episode: 330 Reward: -21.0\n",
            "Episode: 331 Reward: -21.0\n",
            "Episode: 332 Reward: -21.0\n",
            "Episode: 333 Reward: -20.0\n",
            "Episode: 334 Reward: -19.0\n",
            "Episode: 335 Reward: -20.0\n",
            "Episode: 336 Reward: -20.0\n",
            "Episode: 337 Reward: -20.0\n",
            "Episode: 338 Reward: -20.0\n",
            "Episode: 339 Reward: -21.0\n",
            "Episode: 340 Reward: -20.0\n",
            "Episode: 341 Reward: -21.0\n",
            "Episode: 342 Reward: -21.0\n",
            "Episode: 343 Reward: -21.0\n",
            "Episode: 344 Reward: -20.0\n",
            "Episode: 345 Reward: -19.0\n",
            "Episode: 346 Reward: -21.0\n",
            "Episode: 347 Reward: -19.0\n",
            "Episode: 348 Reward: -21.0\n",
            "Episode: 349 Reward: -21.0\n",
            "Episode: 350 Reward: -20.0\n",
            "Episode: 351 Reward: -21.0\n",
            "Episode: 352 Reward: -20.0\n",
            "Episode: 353 Reward: -21.0\n",
            "Episode: 354 Reward: -20.0\n",
            "Episode: 355 Reward: -20.0\n",
            "Episode: 356 Reward: -20.0\n",
            "Episode: 357 Reward: -20.0\n",
            "Episode: 358 Reward: -21.0\n",
            "Episode: 359 Reward: -20.0\n",
            "Episode: 360 Reward: -21.0\n",
            "Episode: 361 Reward: -21.0\n",
            "Episode: 362 Reward: -20.0\n",
            "Episode: 363 Reward: -20.0\n",
            "Episode: 364 Reward: -21.0\n",
            "Episode: 365 Reward: -21.0\n",
            "Episode: 366 Reward: -21.0\n",
            "Episode: 367 Reward: -20.0\n",
            "Episode: 368 Reward: -20.0\n",
            "Episode: 369 Reward: -21.0\n",
            "Episode: 370 Reward: -21.0\n",
            "Episode: 371 Reward: -21.0\n",
            "Episode: 372 Reward: -21.0\n",
            "Episode: 373 Reward: -21.0\n",
            "Episode: 374 Reward: -21.0\n",
            "Episode: 375 Reward: -20.0\n",
            "Episode: 376 Reward: -20.0\n",
            "Episode: 377 Reward: -20.0\n",
            "Episode: 378 Reward: -20.0\n",
            "Episode: 379 Reward: -20.0\n",
            "Episode: 380 Reward: -21.0\n",
            "Episode: 381 Reward: -21.0\n",
            "Episode: 382 Reward: -19.0\n",
            "Episode: 383 Reward: -21.0\n",
            "Episode: 384 Reward: -20.0\n",
            "Episode: 385 Reward: -21.0\n",
            "Episode: 386 Reward: -20.0\n",
            "Episode: 387 Reward: -21.0\n",
            "Episode: 388 Reward: -21.0\n",
            "Episode: 389 Reward: -21.0\n",
            "Episode: 390 Reward: -20.0\n",
            "Episode: 391 Reward: -21.0\n",
            "Episode: 392 Reward: -20.0\n",
            "Episode: 393 Reward: -20.0\n",
            "Episode: 394 Reward: -21.0\n",
            "Episode: 395 Reward: -21.0\n",
            "Episode: 396 Reward: -21.0\n",
            "Episode: 397 Reward: -21.0\n",
            "Episode: 398 Reward: -21.0\n",
            "Episode: 399 Reward: -20.0\n",
            "Episode: 400 Reward: -20.0\n",
            "Episode: 401 Reward: -20.0\n",
            "Episode: 402 Reward: -21.0\n",
            "Episode: 403 Reward: -21.0\n",
            "Episode: 404 Reward: -20.0\n",
            "Episode: 405 Reward: -20.0\n",
            "Episode: 406 Reward: -21.0\n",
            "Episode: 407 Reward: -20.0\n",
            "Episode: 408 Reward: -21.0\n",
            "Episode: 409 Reward: -21.0\n",
            "Episode: 410 Reward: -21.0\n",
            "Episode: 411 Reward: -21.0\n",
            "Episode: 412 Reward: -20.0\n",
            "Episode: 413 Reward: -21.0\n",
            "Episode: 414 Reward: -21.0\n",
            "Episode: 415 Reward: -20.0\n",
            "Episode: 416 Reward: -20.0\n",
            "Episode: 417 Reward: -21.0\n",
            "Episode: 418 Reward: -20.0\n",
            "Episode: 419 Reward: -20.0\n",
            "Episode: 420 Reward: -21.0\n",
            "Episode: 421 Reward: -20.0\n",
            "Episode: 422 Reward: -21.0\n",
            "Episode: 423 Reward: -20.0\n",
            "Episode: 424 Reward: -20.0\n",
            "Episode: 425 Reward: -20.0\n",
            "Episode: 426 Reward: -21.0\n",
            "Episode: 427 Reward: -21.0\n",
            "Episode: 428 Reward: -20.0\n",
            "Episode: 429 Reward: -20.0\n",
            "Episode: 430 Reward: -21.0\n",
            "Episode: 431 Reward: -21.0\n",
            "Episode: 432 Reward: -21.0\n",
            "Episode: 433 Reward: -20.0\n",
            "Episode: 434 Reward: -21.0\n",
            "Episode: 435 Reward: -21.0\n",
            "Episode: 436 Reward: -21.0\n",
            "Episode: 437 Reward: -20.0\n",
            "Episode: 438 Reward: -20.0\n",
            "Episode: 439 Reward: -20.0\n",
            "Episode: 440 Reward: -20.0\n",
            "Episode: 441 Reward: -21.0\n",
            "Episode: 442 Reward: -20.0\n",
            "Episode: 443 Reward: -20.0\n",
            "Episode: 444 Reward: -21.0\n",
            "Episode: 445 Reward: -20.0\n",
            "Episode: 446 Reward: -21.0\n",
            "Episode: 447 Reward: -20.0\n",
            "Episode: 448 Reward: -21.0\n",
            "Episode: 449 Reward: -21.0\n",
            "Episode: 450 Reward: -20.0\n",
            "Episode: 451 Reward: -21.0\n",
            "Episode: 452 Reward: -21.0\n",
            "Episode: 453 Reward: -21.0\n",
            "Episode: 454 Reward: -21.0\n",
            "Episode: 455 Reward: -21.0\n",
            "Episode: 456 Reward: -20.0\n",
            "Episode: 457 Reward: -20.0\n",
            "Episode: 458 Reward: -21.0\n",
            "Episode: 459 Reward: -20.0\n",
            "Episode: 460 Reward: -21.0\n",
            "Episode: 461 Reward: -19.0\n",
            "Episode: 462 Reward: -20.0\n",
            "Episode: 463 Reward: -21.0\n",
            "Episode: 464 Reward: -21.0\n",
            "Episode: 465 Reward: -21.0\n",
            "Episode: 466 Reward: -21.0\n",
            "Episode: 467 Reward: -21.0\n",
            "Episode: 468 Reward: -21.0\n",
            "Episode: 469 Reward: -21.0\n",
            "Episode: 470 Reward: -21.0\n",
            "Episode: 471 Reward: -21.0\n",
            "Episode: 472 Reward: -20.0\n",
            "Episode: 473 Reward: -21.0\n",
            "Episode: 474 Reward: -21.0\n",
            "Episode: 475 Reward: -21.0\n",
            "Episode: 476 Reward: -20.0\n",
            "Episode: 477 Reward: -21.0\n",
            "Episode: 478 Reward: -20.0\n",
            "Episode: 479 Reward: -21.0\n",
            "Episode: 480 Reward: -21.0\n",
            "Episode: 481 Reward: -20.0\n",
            "Episode: 482 Reward: -20.0\n",
            "Episode: 483 Reward: -21.0\n",
            "Episode: 484 Reward: -21.0\n",
            "Episode: 485 Reward: -21.0\n",
            "Episode: 486 Reward: -18.0\n",
            "Episode: 487 Reward: -20.0\n",
            "Episode: 488 Reward: -21.0\n",
            "Episode: 489 Reward: -21.0\n",
            "Episode: 490 Reward: -20.0\n",
            "Episode: 491 Reward: -21.0\n",
            "Episode: 492 Reward: -21.0\n",
            "Episode: 493 Reward: -20.0\n",
            "Episode: 494 Reward: -20.0\n",
            "Episode: 495 Reward: -21.0\n",
            "Episode: 496 Reward: -21.0\n",
            "Episode: 497 Reward: -20.0\n",
            "Episode: 498 Reward: -20.0\n",
            "Episode: 499 Reward: -21.0\n",
            "Episode: 500 Reward: -20.0\n",
            "Episode: 501 Reward: -20.0\n",
            "Episode: 502 Reward: -21.0\n",
            "Episode: 503 Reward: -20.0\n",
            "Episode: 504 Reward: -21.0\n",
            "Episode: 505 Reward: -21.0\n",
            "Episode: 506 Reward: -20.0\n",
            "Episode: 507 Reward: -20.0\n",
            "Episode: 508 Reward: -20.0\n",
            "Episode: 509 Reward: -20.0\n",
            "Episode: 510 Reward: -20.0\n",
            "Episode: 511 Reward: -21.0\n",
            "Episode: 512 Reward: -20.0\n",
            "Episode: 513 Reward: -20.0\n",
            "Episode: 514 Reward: -20.0\n",
            "Episode: 515 Reward: -20.0\n",
            "Episode: 516 Reward: -20.0\n",
            "Episode: 517 Reward: -20.0\n",
            "Episode: 518 Reward: -21.0\n",
            "Episode: 519 Reward: -21.0\n",
            "Episode: 520 Reward: -21.0\n",
            "Episode: 521 Reward: -21.0\n",
            "Episode: 522 Reward: -20.0\n",
            "Episode: 523 Reward: -20.0\n",
            "Episode: 524 Reward: -21.0\n",
            "Episode: 525 Reward: -21.0\n",
            "Episode: 526 Reward: -21.0\n",
            "Episode: 527 Reward: -20.0\n",
            "Episode: 528 Reward: -21.0\n",
            "Episode: 529 Reward: -21.0\n",
            "Episode: 530 Reward: -20.0\n",
            "Episode: 531 Reward: -20.0\n",
            "Episode: 532 Reward: -21.0\n",
            "Episode: 533 Reward: -21.0\n",
            "Episode: 534 Reward: -21.0\n",
            "Episode: 535 Reward: -20.0\n",
            "Episode: 536 Reward: -21.0\n",
            "Episode: 537 Reward: -20.0\n",
            "Episode: 538 Reward: -21.0\n",
            "Episode: 539 Reward: -21.0\n",
            "Episode: 540 Reward: -20.0\n",
            "Episode: 541 Reward: -19.0\n",
            "Episode: 542 Reward: -20.0\n",
            "Episode: 543 Reward: -21.0\n",
            "Episode: 544 Reward: -21.0\n",
            "Episode: 545 Reward: -21.0\n",
            "Episode: 546 Reward: -20.0\n",
            "Episode: 547 Reward: -21.0\n",
            "Episode: 548 Reward: -20.0\n",
            "Episode: 549 Reward: -20.0\n",
            "Episode: 550 Reward: -21.0\n",
            "Episode: 551 Reward: -20.0\n",
            "Episode: 552 Reward: -20.0\n",
            "Episode: 553 Reward: -21.0\n",
            "Episode: 554 Reward: -20.0\n",
            "Episode: 555 Reward: -21.0\n",
            "Episode: 556 Reward: -21.0\n",
            "Episode: 557 Reward: -20.0\n",
            "Episode: 558 Reward: -21.0\n",
            "Episode: 559 Reward: -21.0\n",
            "Episode: 560 Reward: -21.0\n",
            "Episode: 561 Reward: -19.0\n",
            "Episode: 562 Reward: -21.0\n",
            "Episode: 563 Reward: -20.0\n",
            "Episode: 564 Reward: -21.0\n",
            "Episode: 565 Reward: -20.0\n",
            "Episode: 566 Reward: -21.0\n",
            "Episode: 567 Reward: -20.0\n",
            "Episode: 568 Reward: -20.0\n",
            "Episode: 569 Reward: -20.0\n",
            "Episode: 570 Reward: -21.0\n",
            "Episode: 571 Reward: -21.0\n",
            "Episode: 572 Reward: -19.0\n",
            "Episode: 573 Reward: -20.0\n",
            "Episode: 574 Reward: -21.0\n",
            "Episode: 575 Reward: -21.0\n",
            "Episode: 576 Reward: -20.0\n",
            "Episode: 577 Reward: -21.0\n",
            "Episode: 578 Reward: -21.0\n",
            "Episode: 579 Reward: -21.0\n",
            "Episode: 580 Reward: -20.0\n",
            "Episode: 581 Reward: -21.0\n",
            "Episode: 582 Reward: -21.0\n",
            "Episode: 583 Reward: -21.0\n",
            "Episode: 584 Reward: -20.0\n",
            "Episode: 585 Reward: -21.0\n",
            "Episode: 586 Reward: -20.0\n",
            "Episode: 587 Reward: -20.0\n",
            "Episode: 588 Reward: -21.0\n",
            "Episode: 589 Reward: -21.0\n",
            "Episode: 590 Reward: -21.0\n",
            "Episode: 591 Reward: -21.0\n",
            "Episode: 592 Reward: -21.0\n",
            "Episode: 593 Reward: -21.0\n",
            "Episode: 594 Reward: -20.0\n",
            "Episode: 595 Reward: -21.0\n",
            "Episode: 596 Reward: -19.0\n",
            "Episode: 597 Reward: -20.0\n",
            "Episode: 598 Reward: -20.0\n",
            "Episode: 599 Reward: -20.0\n",
            "Episode: 600 Reward: -21.0\n",
            "Episode: 601 Reward: -21.0\n",
            "Episode: 602 Reward: -21.0\n",
            "Episode: 603 Reward: -21.0\n",
            "Episode: 604 Reward: -20.0\n",
            "Episode: 605 Reward: -21.0\n",
            "Episode: 606 Reward: -20.0\n",
            "Episode: 607 Reward: -21.0\n",
            "Episode: 608 Reward: -21.0\n",
            "Episode: 609 Reward: -20.0\n",
            "Episode: 610 Reward: -21.0\n",
            "Episode: 611 Reward: -21.0\n",
            "Episode: 612 Reward: -21.0\n",
            "Episode: 613 Reward: -20.0\n",
            "Episode: 614 Reward: -20.0\n",
            "Episode: 615 Reward: -19.0\n",
            "Episode: 616 Reward: -20.0\n",
            "Episode: 617 Reward: -20.0\n",
            "Episode: 618 Reward: -20.0\n",
            "Episode: 619 Reward: -19.0\n",
            "Episode: 620 Reward: -20.0\n",
            "Episode: 621 Reward: -21.0\n",
            "Episode: 622 Reward: -21.0\n",
            "Episode: 623 Reward: -20.0\n",
            "Episode: 624 Reward: -21.0\n",
            "Episode: 625 Reward: -21.0\n",
            "Episode: 626 Reward: -20.0\n",
            "Episode: 627 Reward: -21.0\n",
            "Episode: 628 Reward: -20.0\n",
            "Episode: 629 Reward: -21.0\n",
            "Episode: 630 Reward: -21.0\n",
            "Episode: 631 Reward: -21.0\n",
            "Episode: 632 Reward: -21.0\n",
            "Episode: 633 Reward: -21.0\n",
            "Episode: 634 Reward: -21.0\n",
            "Episode: 635 Reward: -20.0\n",
            "Episode: 636 Reward: -20.0\n",
            "Episode: 637 Reward: -21.0\n",
            "Episode: 638 Reward: -21.0\n",
            "Episode: 639 Reward: -21.0\n",
            "Episode: 640 Reward: -21.0\n",
            "Episode: 641 Reward: -20.0\n",
            "Episode: 642 Reward: -21.0\n",
            "Episode: 643 Reward: -21.0\n",
            "Episode: 644 Reward: -21.0\n",
            "Episode: 645 Reward: -20.0\n",
            "Episode: 646 Reward: -20.0\n",
            "Episode: 647 Reward: -20.0\n",
            "Episode: 648 Reward: -21.0\n",
            "Episode: 649 Reward: -20.0\n",
            "Episode: 650 Reward: -20.0\n",
            "Episode: 651 Reward: -21.0\n",
            "Episode: 652 Reward: -20.0\n",
            "Episode: 653 Reward: -21.0\n",
            "Episode: 654 Reward: -20.0\n",
            "Episode: 655 Reward: -21.0\n",
            "Episode: 656 Reward: -21.0\n",
            "Episode: 657 Reward: -21.0\n",
            "Episode: 658 Reward: -20.0\n",
            "Episode: 659 Reward: -20.0\n",
            "Episode: 660 Reward: -20.0\n",
            "Episode: 661 Reward: -21.0\n",
            "Episode: 662 Reward: -20.0\n",
            "Episode: 663 Reward: -21.0\n",
            "Episode: 664 Reward: -20.0\n",
            "Episode: 665 Reward: -20.0\n",
            "Episode: 666 Reward: -20.0\n",
            "Episode: 667 Reward: -21.0\n",
            "Episode: 668 Reward: -21.0\n",
            "Episode: 669 Reward: -20.0\n",
            "Episode: 670 Reward: -21.0\n",
            "Episode: 671 Reward: -21.0\n",
            "Episode: 672 Reward: -20.0\n",
            "Episode: 673 Reward: -21.0\n",
            "Episode: 674 Reward: -20.0\n",
            "Episode: 675 Reward: -20.0\n",
            "Episode: 676 Reward: -20.0\n",
            "Episode: 677 Reward: -20.0\n",
            "Episode: 678 Reward: -21.0\n",
            "Episode: 679 Reward: -21.0\n",
            "Episode: 680 Reward: -21.0\n",
            "Episode: 681 Reward: -20.0\n",
            "Episode: 682 Reward: -21.0\n",
            "Episode: 683 Reward: -21.0\n",
            "Episode: 684 Reward: -21.0\n",
            "Episode: 685 Reward: -20.0\n",
            "Episode: 686 Reward: -20.0\n",
            "Episode: 687 Reward: -21.0\n",
            "Episode: 688 Reward: -20.0\n",
            "Episode: 689 Reward: -19.0\n",
            "Episode: 690 Reward: -21.0\n",
            "Episode: 691 Reward: -20.0\n",
            "Episode: 692 Reward: -20.0\n",
            "Episode: 693 Reward: -21.0\n",
            "Episode: 694 Reward: -21.0\n",
            "Episode: 695 Reward: -19.0\n",
            "Episode: 696 Reward: -20.0\n",
            "Episode: 697 Reward: -19.0\n",
            "Episode: 698 Reward: -21.0\n",
            "Episode: 699 Reward: -20.0\n",
            "Episode: 700 Reward: -21.0\n",
            "Episode: 701 Reward: -21.0\n",
            "Episode: 702 Reward: -20.0\n",
            "Episode: 703 Reward: -21.0\n",
            "Episode: 704 Reward: -21.0\n",
            "Episode: 705 Reward: -21.0\n",
            "Episode: 706 Reward: -21.0\n",
            "Episode: 707 Reward: -20.0\n",
            "Episode: 708 Reward: -20.0\n",
            "Episode: 709 Reward: -21.0\n",
            "Episode: 710 Reward: -21.0\n",
            "Episode: 711 Reward: -21.0\n",
            "Episode: 712 Reward: -21.0\n",
            "Episode: 713 Reward: -20.0\n",
            "Episode: 714 Reward: -21.0\n",
            "Episode: 715 Reward: -21.0\n",
            "Episode: 716 Reward: -21.0\n",
            "Episode: 717 Reward: -20.0\n",
            "Episode: 718 Reward: -21.0\n",
            "Episode: 719 Reward: -21.0\n",
            "Episode: 720 Reward: -21.0\n",
            "Episode: 721 Reward: -21.0\n",
            "Episode: 722 Reward: -20.0\n",
            "Episode: 723 Reward: -20.0\n",
            "Episode: 724 Reward: -21.0\n",
            "Episode: 725 Reward: -21.0\n",
            "Episode: 726 Reward: -20.0\n",
            "Episode: 727 Reward: -21.0\n",
            "Episode: 728 Reward: -20.0\n",
            "Episode: 729 Reward: -21.0\n",
            "Episode: 730 Reward: -20.0\n",
            "Episode: 731 Reward: -21.0\n",
            "Episode: 732 Reward: -20.0\n",
            "Episode: 733 Reward: -19.0\n",
            "Episode: 734 Reward: -21.0\n",
            "Episode: 735 Reward: -21.0\n",
            "Episode: 736 Reward: -20.0\n",
            "Episode: 737 Reward: -21.0\n",
            "Episode: 738 Reward: -21.0\n",
            "Episode: 739 Reward: -21.0\n",
            "Episode: 740 Reward: -20.0\n",
            "Episode: 741 Reward: -21.0\n",
            "Episode: 742 Reward: -20.0\n",
            "Episode: 743 Reward: -20.0\n",
            "Episode: 744 Reward: -20.0\n",
            "Episode: 745 Reward: -21.0\n",
            "Episode: 746 Reward: -21.0\n",
            "Episode: 747 Reward: -21.0\n",
            "Episode: 748 Reward: -20.0\n",
            "Episode: 749 Reward: -21.0\n",
            "Episode: 750 Reward: -21.0\n",
            "Episode: 751 Reward: -20.0\n",
            "Episode: 752 Reward: -21.0\n",
            "Episode: 753 Reward: -21.0\n",
            "Episode: 754 Reward: -21.0\n",
            "Episode: 755 Reward: -20.0\n",
            "Episode: 756 Reward: -20.0\n",
            "Episode: 757 Reward: -20.0\n",
            "Episode: 758 Reward: -19.0\n",
            "Episode: 759 Reward: -20.0\n",
            "Episode: 760 Reward: -21.0\n",
            "Episode: 761 Reward: -21.0\n",
            "Episode: 762 Reward: -21.0\n",
            "Episode: 763 Reward: -21.0\n",
            "Episode: 764 Reward: -20.0\n",
            "Episode: 765 Reward: -20.0\n",
            "Episode: 766 Reward: -20.0\n",
            "Episode: 767 Reward: -20.0\n",
            "Episode: 768 Reward: -21.0\n",
            "Episode: 769 Reward: -20.0\n",
            "Episode: 770 Reward: -21.0\n",
            "Episode: 771 Reward: -20.0\n",
            "Episode: 772 Reward: -21.0\n",
            "Episode: 773 Reward: -20.0\n",
            "Episode: 774 Reward: -20.0\n",
            "Episode: 775 Reward: -21.0\n",
            "Episode: 776 Reward: -21.0\n",
            "Episode: 777 Reward: -20.0\n",
            "Episode: 778 Reward: -20.0\n",
            "Episode: 779 Reward: -21.0\n",
            "Episode: 780 Reward: -20.0\n",
            "Episode: 781 Reward: -21.0\n",
            "Episode: 782 Reward: -21.0\n",
            "Episode: 783 Reward: -21.0\n",
            "Episode: 784 Reward: -19.0\n",
            "Episode: 785 Reward: -21.0\n",
            "Episode: 786 Reward: -21.0\n",
            "Episode: 787 Reward: -20.0\n",
            "Episode: 788 Reward: -21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDXR5FiLjqA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 9.** Testing/Visualizing the trained agent/model (Agent plays Pong) "
      ],
      "metadata": {
        "id": "3nEoOik2jqXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output\n",
        "import gym\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Loading the saved/trained model\n",
        "#model = load_model('Pong_PolicyGrad_model_episode_150_2023-03-06_19-13-23.h5')\n",
        "model = load_model('Pong_PolicyGrad_model_episode_750_2023-03-10_14-31-30.h5')\n",
        "\n",
        "# Initializing the environment\n",
        "env = gym.make('Pong-v0')\n",
        "\n",
        "# Playing Pong with saved/trained model\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # Clearing previous frame & rendering the environment\n",
        "    clear_output(wait=True)\n",
        "    cv2_imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "    # Action probabilities for the given state\n",
        "    obs = np.expand_dims(obs, axis=0).astype(np.float32)\n",
        "    logits = model(obs)\n",
        "    action_probabilities = tf.nn.softmax(logits).numpy()[0]\n",
        "\n",
        "    # Sampling action from action probabilities\n",
        "    action = np.random.choice(env.action_space.n, p=action_probabilities)\n",
        "\n",
        "    # Performing action to fetch next state and reward\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "# Finally, closing the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "DJIObBRhjqrz",
        "outputId": "1dfacdba-2f53-482d-aa72-12f9e8035c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=160x210 at 0x7FB439388BE0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACpElEQVR4nO3bwUnDYBiAYSsu4BBu4EXvguAKncJBxCG6gifvenEDV8gQHgTJJWhNY83L85wCTUvg5eNvmr+bkwnnt49TL7Eimzkhr65vvj3n9eX51+ev3d3927fnPD1cLnoNp4t+OkcncJzAcWdz3jy1Xi6xNq/R1Pr6k7X5UExwnMBxAsfNWoOra+cS/nLdHTPBcQLHCRw3aw0eK/2GvITxPbH7YA5G4DiB4w62Bk/dE8/5vXrtjnXvO2aC4wSOEzhu1p4s/j8THCdw3GYYhmNfAwsywXECxwkcJ3CcwHECxwkcJ3CcwHEeNqzAeOPAvn8YN8FxAge977bvu+3n8d57ssZ7qeyF/v9McJzAcQfbNsty9v3mfLHdfR2b4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhu74cNngGviwmOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4zbDMBz7GliQCY4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7guA83Yzg93Np2hgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 10.** Saving Test game played by the trained agent/model (Agent plays Pong) "
      ],
      "metadata": {
        "id": "btuQEcDORfUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgYO9gWJQVfX",
        "outputId": "5ff120a2-9af1-4207-ffbe-897bfdf7f39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from gym.wrappers import RecordVideo\n",
        "\n",
        "# Load trained model\n",
        "#model = load_model('Pong_PolicyGrad_model_episode_50_2023-03-06_14-31-30.h5')\n",
        "model = load_model('Pong_PolicyGrad_model_episode_750_2023-03-10_14-31-30.h5')\n",
        "\n",
        "# Initialize Pong environment\n",
        "env = gym.make('Pong-v0')\n",
        "\n",
        "# Wrap environment to record video\n",
        "env = RecordVideo(env, 'test_video/video.mp4')\n",
        "\n",
        "# Play Pong with trained model\n",
        "obs = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # Render the environment\n",
        "    env.render()\n",
        "\n",
        "    # Get action probabilities for given state\n",
        "    obs = np.expand_dims(obs, axis=0).astype(np.float32)\n",
        "    logits = model(obs)\n",
        "    action_probabilities = tf.nn.softmax(logits).numpy()[0]\n",
        "\n",
        "    # Sample action from action probabilities\n",
        "    action = np.random.choice(env.action_space.n, p=action_probabilities)\n",
        "\n",
        "    # Perform action and get next state and reward\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8TAuhPnPStR",
        "outputId": "6aad8903-710f-40e8-cb1d-719afb96cf2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/test_video/video.mp4 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment Pong-v0 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    }
  ]
}